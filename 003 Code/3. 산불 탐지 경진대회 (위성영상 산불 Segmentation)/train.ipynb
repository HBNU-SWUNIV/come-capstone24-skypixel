{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리 설치 (Python 3.8) 및 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import requests\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "from PIL import Image\n",
    "import sys\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from segmentation_models_pytorch.utils.metrics import IoU\n",
    "import wandb\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 경로\n",
    "train_img_path = '../dataset/train_img'\n",
    "train_mask_path = '../dataset/train_mask'\n",
    "\n",
    "# 생성 폴더 경로\n",
    "train_split_path = './dataset/train_split'\n",
    "model_save_path = 'model_save_path'\n",
    "\n",
    "os.makedirs(train_split_path, exist_ok=True)\n",
    "os.makedirs(model_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 params\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 256                                                               \n",
    "CHANNELS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_WORKERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 설정\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed) \n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed) \n",
    "torch.manual_seed(seed)  \n",
    "torch.cuda.manual_seed(seed)  \n",
    "torch.backends.cudnn.benchmark = False  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(images_dir, masks_dir, output_dir, test_size, seed):\n",
    "    image_files = sorted([file for file in os.listdir(images_dir) if file.endswith('.tif')])\n",
    "    mask_files = sorted([file for file in os.listdir(masks_dir) if file.endswith('.tif')])\n",
    "\n",
    "    # 분할 실행\n",
    "    train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "        image_files, mask_files, test_size=test_size, random_state=seed)\n",
    "\n",
    "    # 디렉토리 설정\n",
    "    train_images_dir = os.path.join(output_dir, 'train/images')\n",
    "    train_masks_dir = os.path.join(output_dir, 'train/masks')\n",
    "    val_images_dir = os.path.join(output_dir, 'val/images')\n",
    "    val_masks_dir = os.path.join(output_dir, 'val/masks')\n",
    "\n",
    "    # 디렉토리 생성\n",
    "    os.makedirs(train_images_dir, exist_ok=True)\n",
    "    os.makedirs(train_masks_dir, exist_ok=True)\n",
    "    os.makedirs(val_images_dir, exist_ok=True)\n",
    "    os.makedirs(val_masks_dir, exist_ok=True)\n",
    "\n",
    "    # ThreadPoolExecutor를 사용하여 파일 복사 작업을 병렬로 실행\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        executor.map(shutil.copy, [os.path.join(images_dir, image) for image in train_images],\n",
    "                     [os.path.join(train_images_dir, image) for image in train_images])\n",
    "        executor.map(shutil.copy, [os.path.join(masks_dir, mask) for mask in train_masks],\n",
    "                     [os.path.join(train_masks_dir, mask) for mask in train_masks])\n",
    "        executor.map(shutil.copy, [os.path.join(images_dir, image) for image in val_images],\n",
    "                     [os.path.join(val_images_dir, image) for image in val_images])\n",
    "        executor.map(shutil.copy, [os.path.join(masks_dir, mask) for mask in val_masks],\n",
    "                     [os.path.join(val_masks_dir, mask) for mask in val_masks])\n",
    "        \n",
    "split_data(train_img_path, train_mask_path, train_split_path, test_size=0.2, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounds(width, height, transform):\n",
    "\n",
    "    left = int(float(transform[2]))\n",
    "    right = int(float(transform[2])) + int(float(width))*int(float(transform[0]))\n",
    "    bottom = int(float(transform[5])) + int(float(height))*int(float(transform[4]))\n",
    "    top = int(float(transform[5]))\n",
    "\n",
    "    bounds = (left, bottom, right, top)\n",
    "\n",
    "    return bounds\n",
    "\n",
    "def get_extent(dataset):\n",
    "\n",
    "    cols = dataset.RasterXSize\n",
    "    rows = dataset.RasterYSize\n",
    "    transform = dataset.GetGeoTransform()\n",
    "\n",
    "    minx = transform[0]\n",
    "    maxx = transform[0] + (cols * transform[1]) + (rows * transform[2])\n",
    "    miny = transform[3] + (cols * transform[4]) + (rows * transform[5])\n",
    "    maxy = transform[3]\n",
    "\n",
    "    return {\"minX\": str(minx), \"maxX\": str(maxx),\n",
    "            \"minY\": str(miny), \"maxY\": str(maxy),\n",
    "            \"cols\": str(cols), \"rows\": str(rows)}\n",
    "\n",
    "def getReflectance (band, add_band, mult_band, sun_elevation):\n",
    "    p = ((band * mult_band) + add_band)\n",
    "    corrected = p / math.sin (math.radians (sun_elevation))\n",
    "\n",
    "    return p, corrected\n",
    "\n",
    "def get_saturation(BQA):\n",
    "    vals = [2724,2756,2804,2980,3012,3748,3780,6820,6852,6900,7076,7108,7844,7876,\n",
    "            2728,2760,2808,2984,3016,3752,3784,6824,6856,6904,7080,7112,7848,7880,\n",
    "            2732,2764,2812,2988,3020,3756,3788,6828,6860,6908,7084,7116,7852,7884]\n",
    "    \n",
    "    sat = np.zeros((BQA.shape), dtype=bool)\n",
    "\n",
    "    for val in vals:\n",
    "        sat = sat | (BQA==val)\n",
    "        \n",
    "    return sat.astype(int)\n",
    "\n",
    "def Seq1 (bands, r75, diff75):\n",
    "    return (np.logical_and (bands [7] > 0.5, np.logical_and (r75 > 2.5, diff75 > 0.3)))\n",
    "\n",
    "def Seq2 (bands):\n",
    "    return (np.logical_and (bands [6] > 0.8, np.logical_and (bands [1] < 0.2, np.logical_or (bands [5] > 0.4, bands [7] < 0.1))))\n",
    "\n",
    "def Seq3 (r75, diff75):\n",
    "    return (np.logical_and (r75 > 1.8, diff75 > 0.17))\n",
    "\n",
    "def Seq4and5 (bands, r75, unamb_fires, potential_fires, water):\n",
    "\n",
    "    ignored_pixels = np.logical_or (bands [7] <= 0, np.logical_or (unamb_fires, water))\n",
    "    kept_pixels = np.logical_not (ignored_pixels)\n",
    "\n",
    "    r75_ignored = r75.copy ()\n",
    "    r75_ignored [ignored_pixels] = np.nan\n",
    "\n",
    "    band7_ignored = bands [7].copy ()\n",
    "    band7_ignored [ignored_pixels] = np.nan\n",
    "\n",
    "    candidates = np.nonzero (potential_fires)\n",
    "    for i in range (len (candidates [0])):\n",
    "        y = candidates [0][i]\n",
    "        x = candidates [1][i]\n",
    "\n",
    "        t = max (0,y-30)\n",
    "        b = min (potential_fires.shape [0], y+31)\n",
    "        l = max (0, x-30)\n",
    "        r = min (potential_fires.shape [1], x+31)\n",
    "\n",
    "        eq4_result = r75 [y,x] > np.nanmean (r75_ignored [t:b,l:r]) + np.maximum (3 * (np.nanstd (r75_ignored [t:b,l:r])), 0.8)\n",
    "        eq5_result = bands [7][y,x] > np.nanmean (band7_ignored [t:b,l:r]) + np.maximum (3 * (np.nanstd (band7_ignored [t:b,l:r])), 0.08)\n",
    "        if not (eq4_result) or not (eq5_result):\n",
    "            potential_fires [y,x] = False\n",
    "\n",
    "    return potential_fires\n",
    "\n",
    "def Seq6 (bands):\n",
    "    p6 = np.where (bands[6] == 0, np.finfo (float).eps, bands[6])\n",
    "    return (bands [7] / p6 > 1.6)\n",
    "\n",
    "def Seq7_8_9 (bands):\n",
    "    result7 = np.logical_and (bands [4] > bands [5], np.logical_and (bands [5] > bands [6], np.logical_and (bands [6] > bands [7], bands [1] - bands [7] < 0.2)))\n",
    "    return (np.logical_and (result7, np.logical_or (bands [3] > bands [2], np.logical_and (bands [1] > bands [2], np.logical_and (bands [2] > bands [3], bands [3] > bands [4])))))\n",
    "\n",
    "\n",
    "def Geq12 (bands):\n",
    "    return (bands [4] <= 0.53 * bands [7] - 0.214)\n",
    "\n",
    "def Geq13 (bands, eq12_mask):\n",
    "    neighborhood = cv2.dilate (eq12_mask.astype (np.uint8), cv2.getStructuringElement (cv2.MORPH_RECT, (3,3))).astype (eq12_mask.dtype)\n",
    "\n",
    "    return (np.logical_and (neighborhood, bands [4] <= 0.35 * bands [6] - 0.044))\n",
    "\n",
    "def Geq14 (bands):\n",
    "    return (bands [4] <= 0.53 * bands [7] - 0.125)\n",
    "\n",
    "def Geq15 (bands):\n",
    "    return (bands [6] <= 1.08 * bands [7] - 0.048)\n",
    "\n",
    "def Geq16 (bands):\n",
    "    return (np.logical_and (np.logical_and (bands [2] > bands [3], bands [3] > bands [4]), bands [4] > bands [5]))\n",
    "\n",
    "def pixelVal(p7,ef,ep,ew):\n",
    "    e = np.logical_and (p7>0, np.logical_and (np.logical_not (ef), np.logical_and (np.logical_not (ep), np.logical_not (ew))))\n",
    "    return e\n",
    "\n",
    "def Geq8and9 (bands, valid, unamb_fires, potential_fires, water):\n",
    "\n",
    "    ignored_pixels = np.logical_or (unamb_fires, np.logical_or (potential_fires, water))\n",
    "    ignored_pixels = np.logical_or (ignored_pixels, np.logical_not (valid))\n",
    "    kept_pixels = np.logical_not (ignored_pixels)\n",
    "\n",
    "    r75 = bands [7] / bands [5]\n",
    "    r75_ignored = r75.copy ()\n",
    "    r75_ignored [ignored_pixels] = np.nan\n",
    "\n",
    "    band7_ignored = bands [7].copy ()\n",
    "    band7_ignored [ignored_pixels] = np.nan\n",
    "\n",
    "    sizes = list(range(5,61+2,2))\n",
    "\n",
    "    candidates = np.nonzero (potential_fires)\n",
    "\n",
    "    for i in range (len (candidates [0])):\n",
    "        y = candidates [0][i]\n",
    "        x = candidates [1][i]\n",
    "        tested = False\n",
    "        for w in sizes:\n",
    "            t = max (0,y-w//2)\n",
    "            b = min (potential_fires.shape [0], y+w//2+1)\n",
    "            l = max (0, x-w//2)\n",
    "            r = min (potential_fires.shape [1], x+w//2+1)\n",
    "\n",
    "            if np.count_nonzero (kept_pixels [t:b,l:r]) >= 0.25 * (b-t)*(r-l):\n",
    "                tested = True\n",
    "                eq8_result = r75 [y,x] > np.nanmean (r75_ignored [t:b,l:r]) + np.maximum (3 * (np.nanstd (r75_ignored [t:b,l:r])), 0.8)\n",
    "                eq9_result = bands [7][y,x] > np.nanmean (band7_ignored [t:b,l:r]) + np.maximum (3 * (np.nanstd (band7_ignored [t:b,l:r])), 0.08)\n",
    "                if not (eq8_result) or not (eq9_result):\n",
    "                    potential_fires [y,x] = False\n",
    "                break\n",
    "\n",
    "        if not tested:\n",
    "            potential_fires [y,x] = False\n",
    "\n",
    "    return potential_fires\n",
    "\n",
    "def Meq2 (bands):\n",
    "\n",
    "    p5 = np.where (bands[5] == 0, np.finfo (float).eps, bands[5])\n",
    "    p6 = np.where (bands[6] == 0, np.finfo (float).eps, bands[6])\n",
    "    \n",
    "    return (np.logical_and (bands[7] >= 0.15, np.logical_and (bands[7]/p6 >= 1.4, bands[7]/p5 >= 1.4)))\n",
    "\n",
    "def Meq3 (bands, unamb, sat):\n",
    "\n",
    "    neighborhood = cv2.dilate (unamb.astype (np.uint8), cv2.getStructuringElement (cv2.MORPH_RECT, (3,3))).astype (unamb.dtype)\n",
    "    p5 = np.where (bands[5] > 0, np.finfo (float).eps, bands[5])\n",
    "    \n",
    "    return (np.logical_and (neighborhood, np.logical_or (np.logical_and (bands[6]/p5 >= 2.0, bands[6]>=0.5), sat)))\n",
    "\n",
    "def getFireGOLI (bands):\n",
    "\n",
    "    valid = bands [7] > 0\n",
    "    valid = cv2.erode (valid.astype (np.uint8), cv2.getStructuringElement (cv2.MORPH_RECT, (3,3))).astype (np.uint8)\n",
    "\n",
    "    unamb_fires = Geq12 (bands)\n",
    "    unamb_fires = np.logical_and (valid, unamb_fires)\n",
    "    if np.any (unamb_fires):\n",
    "        unamb_fires = np.logical_or (unamb_fires, Geq13 (bands, unamb_fires))\n",
    "        unamb_fires = np.logical_and (valid, unamb_fires)\n",
    "\n",
    "    potential_fires = Geq14 (bands)\n",
    "    potential_fires = np.logical_or (potential_fires, Geq15 (bands))\n",
    "    potential_fires = np.logical_and (valid, potential_fires)\n",
    "\n",
    "    water = Geq16 (bands)\n",
    "\n",
    "    if np.any (potential_fires):\n",
    "        potential_fires = Geq8and9 (bands, valid, unamb_fires, potential_fires, water)\n",
    "\n",
    "    scaled_band = np.logical_and (np.logical_or (unamb_fires, potential_fires), np.logical_not (water))\n",
    "    return (scaled_band.astype (int))\n",
    "\n",
    "def getFireMurphy (bands, saturated):\n",
    "    unamb_fires = Meq2 (bands)\n",
    "\n",
    "    if np.any (unamb_fires):\n",
    "        potential_fires = Meq3 (bands, unamb_fires, saturated)\n",
    "        scaled_band = (unamb_fires | potential_fires)\n",
    "    else:\n",
    "        scaled_band = unamb_fires\n",
    "\n",
    "    return (scaled_band.astype (int))\n",
    "\n",
    "def getFireSchroeder (bands):\n",
    "    r75 = bands [7] / bands [5]\n",
    "    diff75 = bands [7] - bands [5]\n",
    "\n",
    "    unamb_fires = Seq1 (bands, r75, diff75)\n",
    "    unamb_fires = np.logical_or (unamb_fires, Seq2 (bands))\n",
    "\n",
    "    potential_fires = Seq3 (r75, diff75)\n",
    "\n",
    "    potential_fires = np.logical_and (potential_fires, Seq6 (bands))\n",
    "\n",
    "    water = Seq7_8_9 (bands)\n",
    "\n",
    "    if np.any (potential_fires):\n",
    "        potential_fires = Seq4and5 (bands, r75, unamb_fires, potential_fires, water)\n",
    "\n",
    "    scaled_band = np.logical_and (np.logical_or (unamb_fires, potential_fires), np.logical_not (water))\n",
    "    return (scaled_band.astype (int))\n",
    "\n",
    "def processImage (in_dir, image_name, Aref, Mref, SE, sat):\n",
    "    bands = np.zeros((8, 256, 256))\n",
    "    \n",
    "    with rasterio.open (os.path.join (in_dir, image_name + '.tif')) as src:\n",
    "        profile = src.profile.copy ()\n",
    "        for i in range (8):\n",
    "            if i == 0:\n",
    "                pass\n",
    "            else:\n",
    "                bands[i] = src.read (i)\n",
    "                \n",
    "    reflectance = np.copy(bands)\n",
    "    corrected = np.copy(bands)\n",
    "    \n",
    "    for i in range (1,8):\n",
    "        reflectance[i], corrected[i] = getReflectance (bands[i], Aref[i-1], Mref[i-1], SE)\n",
    "        \n",
    "    scaled_schroeder = getFireSchroeder(reflectance)\n",
    "    scaled_goli = getFireGOLI(corrected)\n",
    "    scaled_murphy = getFireMurphy(corrected, sat)\n",
    "\n",
    "    scaled_ir_band = [scaled_schroeder, scaled_goli, scaled_murphy]\n",
    "    \n",
    "    return np.array(scaled_ir_band)\n",
    "\n",
    "def voted_image(scaled_ir_band):\n",
    "    ir_voted_band = np.sum(scaled_ir_band, axis=0) >= 2\n",
    "    ir_voted_band = ir_voted_band.astype(np.uint8)\n",
    "    \n",
    "    return ir_voted_band\n",
    "\n",
    "def bands_combine(in_dir, image_name, scaling_band, g_band, b_band):\n",
    "    tif_file = os.path.join (in_dir, image_name + '.tif')\n",
    "    tif_data = gdal.Open(tif_file)\n",
    "    \n",
    "    band_b = (tif_data.GetRasterBand(1).ReadAsArray() / 256).astype(np.uint8)\n",
    "    band_g = (tif_data.GetRasterBand(2).ReadAsArray() / 256).astype(np.uint8)\n",
    "    band_r = scaling_band * 255\n",
    "    \n",
    "    rgb_image = np.dstack((band_b, band_g, band_r)).astype(np.uint8)\n",
    "    \n",
    "    return rgb_image\n",
    "\n",
    "def adjust_contrast(image, contrast=1.0):\n",
    "    f = 131 * (contrast + 127) / (127 * (131 - contrast))\n",
    "    alpha_c = f\n",
    "    gamma_c = 127 * (1 - f)\n",
    "    return cv2.addWeighted(image, alpha_c, image, 0, gamma_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGenerator(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None, percent=1, ASSUMED_SE=55, ASSUMED_MREF=None, ASSUMED_AREF=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.images = sorted([x for x in os.listdir(images_dir) if x.endswith('.tif')])[:int(len(os.listdir(images_dir)) * percent)]\n",
    "        self.masks = sorted([x for x in os.listdir(masks_dir) if x.endswith('.tif')])[:int(len(os.listdir(masks_dir)) * percent)]\n",
    "        self.transform = transform\n",
    "        self.ASSUMED_SE = ASSUMED_SE\n",
    "        self.ASSUMED_MREF = [2e-05] * 8 if ASSUMED_MREF is None else ASSUMED_MREF\n",
    "        self.ASSUMED_AREF = [-0.1] * 8 if ASSUMED_AREF is None else ASSUMED_AREF\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tif_path = os.path.join(self.images_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.masks_dir, self.masks[idx])\n",
    "        \n",
    "        image_name = os.path.basename(tif_path.replace('.tif',''))\n",
    "        with rasterio.open(tif_path) as src:\n",
    "            profile = src.profile.copy()\n",
    "            BQA = src.read(1)\n",
    "        saturation = get_saturation(BQA)  \n",
    "        scaling_band_3ch = processImage(self.images_dir, image_name, self.ASSUMED_AREF, self.ASSUMED_MREF, self.ASSUMED_SE, saturation)\n",
    "        scaling_band = voted_image(scaling_band_3ch)\n",
    "        img_array = bands_combine(self.images_dir, image_name, scaling_band, 2, 1)\n",
    "        img_array = adjust_contrast(img_array, contrast=1.3)\n",
    "        img = Image.fromarray(img_array).convert(\"RGB\")\n",
    "        \n",
    "        with rasterio.open(mask_path) as mask_src:\n",
    "            mask_array = mask_src.read(1)\n",
    "            mask_array = np.where(mask_array > 0, 1, 0).astype(np.uint8)\n",
    "        \n",
    "        mask = Image.fromarray(mask_array * 255).convert(\"L\")\n",
    "\n",
    "        sample = {'image': img, 'mask': mask}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "        to_tensor_transform = transforms.ToTensor()\n",
    "        image = to_tensor_transform(image)\n",
    "        mask = to_tensor_transform(mask)\n",
    "        return {'image': image, 'mask': mask}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out, dilation=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channel_in, channel_out, kernel_size=3, padding='same', dilation=dilation)\n",
    "        self.conv2 = nn.Conv2d(channel_out, channel_out, kernel_size=3, padding='same', dilation=dilation)\n",
    "        self.bnorm1 = nn.BatchNorm2d(channel_out)\n",
    "        self.bnorm2 = nn.BatchNorm2d(channel_out)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        conv1 = self.activation(self.bnorm1(conv1))\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv2 = self.activation(self.bnorm2(conv2))\n",
    "        return conv2\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, channel_in):\n",
    "        super().__init__()\n",
    "        self.downsample = nn.Conv2d(channel_in, channel_in * 2, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.downsample(x)\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out):\n",
    "        super().__init__()\n",
    "        self.conv_transpose = nn.ConvTranspose2d(channel_in, channel_out, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_transpose(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, clannels, classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.CHANNELS = clannels\n",
    "        self.CLASSES = classes\n",
    "\n",
    "        self.inp = ConvBlock(self.CHANNELS, 64)\n",
    "\n",
    "        self.stage1 = ConvBlock(128, 128, dilation=1)\n",
    "        self.stage2 = ConvBlock(256, 256, dilation=1)\n",
    "        self.stage3 = ConvBlock(512, 512, dilation=2)\n",
    "        self.stage4 = ConvBlock(1024, 1024, dilation=3)\n",
    "\n",
    "        self.down1 = Downsample(64)\n",
    "        self.down2 = Downsample(128)\n",
    "        self.down3 = Downsample(256)\n",
    "        self.down4 = Downsample(512)\n",
    "\n",
    "        self.up1 = Upsample(1024, 512)\n",
    "        self.up2 = Upsample(512, 256)\n",
    "        self.up3 = Upsample(256, 128)\n",
    "        self.up4 = Upsample(128, 64)\n",
    "\n",
    "        self.stage4i = ConvBlock(1024, 512, dilation=3)\n",
    "        self.stage3i = ConvBlock(512, 256, dilation=2)\n",
    "        self.stage2i = ConvBlock(256, 128, dilation=1)\n",
    "        self.stage1i = ConvBlock(128, 64, dilation=1)\n",
    "\n",
    "        self.out = nn.Conv2d(64, self.CLASSES, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.inp(x)\n",
    "        d1 = self.down1(a1)\n",
    "\n",
    "        a2 = self.stage1(d1)\n",
    "        d2 = self.down2(a2)\n",
    "\n",
    "        a3 = self.stage2(d2)\n",
    "        d3 = self.down3(a3)\n",
    "\n",
    "        a4 = self.stage3(d3)\n",
    "        d4 = self.down4(a4)\n",
    "\n",
    "        a5 = self.stage4(d4)\n",
    "        u1 = self.up1(a5)\n",
    "\n",
    "        c1 = self.stage4i(torch.cat([a4, u1], dim=1))\n",
    "        u2 = self.up2(c1)\n",
    "\n",
    "        c2 = self.stage3i(torch.cat([a3, u2], dim=1))\n",
    "        u3 = self.up3(c2)\n",
    "\n",
    "        c3 = self.stage2i(torch.cat([a2, u3], dim=1))\n",
    "        u4 = self.up4(c3)\n",
    "\n",
    "        c4 = self.stage1i(torch.cat([a1, u4], dim=1))\n",
    "        logits = self.out(c4)\n",
    "\n",
    "        return logits\n",
    "\n",
    "def unet(n_channels=3, n_classes=1):\n",
    "    return UNet(n_channels, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dice_loss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(dice_loss, self).__init__()\n",
    "        self.smooth = 1.\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "       logf = torch.sigmoid(logits).view(-1)\n",
    "       labf = labels.view(-1)\n",
    "       intersection = (logf * labf).sum()\n",
    "\n",
    "       num = 2. * intersection + self.smooth\n",
    "       den = logf.sum() + labf.sum() + self.smooth\n",
    "       return 1 - (num/den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader():\n",
    "    trainDataset = CustomDataGenerator(images_dir=train_split_path + '/train/images',\n",
    "                                    masks_dir=train_split_path + '/train/masks', \n",
    "                                    percent=1.0,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        ToTensor(),\n",
    "                                        ])\n",
    "                                    )\n",
    "\n",
    "    # no transforms here, just resize the image\n",
    "    valDataset = CustomDataGenerator(images_dir=train_split_path + '/val/images',\n",
    "                                    masks_dir=train_split_path + '/val/masks',\n",
    "                                    percent=1.0,\n",
    "                                    transform=transforms.Compose([\n",
    "                                        ToTensor(),\n",
    "                                        ])\n",
    "                                    )\n",
    "\n",
    "    trainLoader = DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "    valLoader = DataLoader(valDataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    return iter(trainLoader), iter(valLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_metric = IoU(threshold=0.5, ignore_channels=None)\n",
    "\n",
    "def train(model, criterion, opt):\n",
    "    global EPOCHS, BATCH_SIZE, model_save_path, device\n",
    "    model.train()\n",
    "\n",
    "    best_val_miou = 0.0 \n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loader, val_loader = get_dataloader()\n",
    "\n",
    "        train_loss, train_iou_scores, val_loss, val_iou_scores = 0.0, [], 0.0, []\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            images, labels = batch['image'].to(device), batch['mask'].to(device)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "\n",
    "            outputs = torch.sigmoid(outputs) > 0.5  \n",
    "            iou_score = iou_metric(outputs, labels)\n",
    "            train_iou_scores.append(iou_score.item())\n",
    "\n",
    "            num_batches += 1\n",
    "        \n",
    "        train_loss /= num_batches\n",
    "        avg_train_iou = sum(train_iou_scores) / num_batches\n",
    "\n",
    "        model.eval()\n",
    "        num_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, total=len(val_loader)):\n",
    "                images, labels = batch['image'].to(device), batch['mask'].to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                outputs = torch.sigmoid(outputs) > 0.5  \n",
    "                iou_score = iou_metric(outputs, labels)\n",
    "                val_iou_scores.append(iou_score.item())\n",
    "\n",
    "                num_batches += 1\n",
    "            \n",
    "            val_loss /= num_batches\n",
    "            avg_val_iou = sum(val_iou_scores) / num_batches\n",
    "            \n",
    "            if avg_val_iou >= best_val_miou:\n",
    "                best_val_miou = avg_val_iou\n",
    "                torch.save(model.state_dict(), f\"{model_save_path}/best_model.pth\")\n",
    "\n",
    "        # wandb.log({\n",
    "        #     \"Train Loss\": train_loss, \n",
    "        #     \"Train mIoU\": avg_train_iou, \n",
    "        #     \"Val Loss\": val_loss, \n",
    "        #     \"Val mIoU\": avg_val_iou\n",
    "        # })\n",
    "\n",
    "        torch.save(model.state_dict(), f\"{model_save_path}/model_epoch_{epoch+1}.pth\")\n",
    "        print(f\"Epoch {epoch+1} completed. Train Loss: {train_loss:.4f}, Train mIoU: {avg_train_iou:.4f}, Val Loss: {val_loss:.4f}, Val mIoU: {avg_val_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project=wandb_project_name)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/840 [00:00<?, ?it/s]c:\\Users\\user\\anaconda3\\envs\\torch\\Lib\\site-packages\\osgeo\\gdal.py:312: FutureWarning: Neither gdal.UseExceptions() nor gdal.DontUseExceptions() has been explicitly called. In GDAL 4.0, exceptions will be enabled by default.\n",
      "  warnings.warn(\n",
      "  4%|▍         | 35/840 [00:59<22:22,  1.67s/it]C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_9692\\3265270166.py:201: RuntimeWarning: divide by zero encountered in divide\n",
      "  r75 = bands [7] / bands [5]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_9692\\3265270166.py:118: RuntimeWarning: divide by zero encountered in divide\n",
      "  r75 = bands [7] / bands [5]\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_9692\\3265270166.py:164: RuntimeWarning: divide by zero encountered in divide\n",
      "  return (np.logical_and (neighborhood, np.logical_or (np.logical_and (bands[6]/p5 >= 2.0, bands[6]>=0.5), sat)))\n",
      "100%|██████████| 840/840 [25:15<00:00,  1.80s/it]\n",
      "100%|██████████| 210/210 [05:29<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved with Val mIoU: 0.9773\n",
      "Epoch 1 completed. Train Loss: 0.1808, Train mIoU: 0.8523, Val Loss: 0.0138, Val mIoU: 0.9773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 840/840 [25:33<00:00,  1.83s/it]\n",
      "100%|██████████| 210/210 [05:18<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Train Loss: 0.0111, Train mIoU: 0.9783, Val Loss: 0.0115, Val mIoU: 0.9773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 840/840 [23:37<00:00,  1.69s/it]\n",
      "100%|██████████| 210/210 [04:57<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed. Train Loss: 0.0114, Train mIoU: 0.9778, Val Loss: 0.0115, Val mIoU: 0.9773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 840/840 [23:37<00:00,  1.69s/it]\n",
      "100%|██████████| 210/210 [04:58<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed. Train Loss: 0.0112, Train mIoU: 0.9782, Val Loss: 0.0115, Val mIoU: 0.9773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 840/840 [23:42<00:00,  1.69s/it]\n",
      "100%|██████████| 210/210 [04:42<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 completed. Train Loss: 0.0113, Train mIoU: 0.9779, Val Loss: 0.0115, Val mIoU: 0.9773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 840/840 [24:29<00:00,  1.75s/it]\n",
      "100%|██████████| 210/210 [05:07<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 completed. Train Loss: 0.0113, Train mIoU: 0.9780, Val Loss: 0.0115, Val mIoU: 0.9773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 840/840 [24:53<00:00,  1.78s/it]\n",
      "100%|██████████| 210/210 [05:10<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 completed. Train Loss: 0.0113, Train mIoU: 0.9779, Val Loss: 0.0115, Val mIoU: 0.9773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 840/840 [24:27<00:00,  1.75s/it]\n",
      "100%|██████████| 210/210 [04:46<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 completed. Train Loss: 0.0112, Train mIoU: 0.9782, Val Loss: 0.0115, Val mIoU: 0.9773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 840/840 [24:02<00:00,  1.72s/it]\n",
      "100%|██████████| 210/210 [05:01<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 completed. Train Loss: 0.0113, Train mIoU: 0.9780, Val Loss: 0.0115, Val mIoU: 0.9773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 840/840 [24:10<00:00,  1.73s/it]\n",
      "100%|██████████| 210/210 [04:52<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 completed. Train Loss: 0.0113, Train mIoU: 0.9780, Val Loss: 0.0115, Val mIoU: 0.9773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = unet(n_channels=CHANNELS).to(device)\n",
    "# model = nn.DataParallel(model) # GPU 병렬 학습\n",
    "\n",
    "criterion = dice_loss()\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "train(model, criterion, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
