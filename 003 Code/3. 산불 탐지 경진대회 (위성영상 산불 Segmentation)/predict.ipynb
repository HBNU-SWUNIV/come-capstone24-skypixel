{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import requests\n",
    "from osgeo import gdal\n",
    "import rasterio\n",
    "from PIL import Image\n",
    "import sys\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from segmentation_models_pytorch.utils.metrics import IoU\n",
    "import wandb\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "from torchvision import transforms, utils\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 경로\n",
    "predict_img_path = '../dataset/test_img'\n",
    "predict_pth_path = './model_save/best_model.pth'\n",
    "\n",
    "# 생성 폴더 경로\n",
    "predict_mask_path = './predict_mask'\n",
    "pkl_save_path = './pkl_save'\n",
    "\n",
    "os.makedirs(predict_mask_path, exist_ok=True)\n",
    "os.makedirs(pkl_save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 params\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 256\n",
    "CHANNELS = 3\n",
    "NUM_WORKERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 설정\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed) \n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed) \n",
    "torch.manual_seed(seed)  \n",
    "torch.cuda.manual_seed(seed)  \n",
    "torch.backends.cudnn.benchmark = False  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounds(width, height, transform):\n",
    "\n",
    "    left = int(float(transform[2]))\n",
    "    right = int(float(transform[2])) + int(float(width))*int(float(transform[0]))\n",
    "    bottom = int(float(transform[5])) + int(float(height))*int(float(transform[4]))\n",
    "    top = int(float(transform[5]))\n",
    "\n",
    "    bounds = (left, bottom, right, top)\n",
    "\n",
    "    return bounds\n",
    "\n",
    "def get_extent(dataset):\n",
    "\n",
    "    cols = dataset.RasterXSize\n",
    "    rows = dataset.RasterYSize\n",
    "    transform = dataset.GetGeoTransform()\n",
    "\n",
    "    minx = transform[0]\n",
    "    maxx = transform[0] + (cols * transform[1]) + (rows * transform[2])\n",
    "    miny = transform[3] + (cols * transform[4]) + (rows * transform[5])\n",
    "    maxy = transform[3]\n",
    "\n",
    "    return {\"minX\": str(minx), \"maxX\": str(maxx),\n",
    "            \"minY\": str(miny), \"maxY\": str(maxy),\n",
    "            \"cols\": str(cols), \"rows\": str(rows)}\n",
    "\n",
    "def getReflectance (band, add_band, mult_band, sun_elevation):\n",
    "    p = ((band * mult_band) + add_band)\n",
    "    corrected = p / math.sin (math.radians (sun_elevation))\n",
    "\n",
    "    return p, corrected\n",
    "\n",
    "def get_saturation(BQA):\n",
    "    vals = [2724,2756,2804,2980,3012,3748,3780,6820,6852,6900,7076,7108,7844,7876,\n",
    "            2728,2760,2808,2984,3016,3752,3784,6824,6856,6904,7080,7112,7848,7880,\n",
    "            2732,2764,2812,2988,3020,3756,3788,6828,6860,6908,7084,7116,7852,7884]\n",
    "    \n",
    "    sat = np.zeros((BQA.shape), dtype=bool)\n",
    "\n",
    "    for val in vals:\n",
    "        sat = sat | (BQA==val)\n",
    "        \n",
    "    return sat.astype(int)\n",
    "\n",
    "def Seq1 (bands, r75, diff75):\n",
    "    return (np.logical_and (bands [7] > 0.5, np.logical_and (r75 > 2.5, diff75 > 0.3)))\n",
    "\n",
    "def Seq2 (bands):\n",
    "    return (np.logical_and (bands [6] > 0.8, np.logical_and (bands [1] < 0.2, np.logical_or (bands [5] > 0.4, bands [7] < 0.1))))\n",
    "\n",
    "def Seq3 (r75, diff75):\n",
    "    return (np.logical_and (r75 > 1.8, diff75 > 0.17))\n",
    "\n",
    "def Seq4and5 (bands, r75, unamb_fires, potential_fires, water):\n",
    "\n",
    "    ignored_pixels = np.logical_or (bands [7] <= 0, np.logical_or (unamb_fires, water))\n",
    "    kept_pixels = np.logical_not (ignored_pixels)\n",
    "\n",
    "    r75_ignored = r75.copy ()\n",
    "    r75_ignored [ignored_pixels] = np.nan\n",
    "\n",
    "    band7_ignored = bands [7].copy ()\n",
    "    band7_ignored [ignored_pixels] = np.nan\n",
    "\n",
    "    candidates = np.nonzero (potential_fires)\n",
    "    for i in range (len (candidates [0])):\n",
    "        y = candidates [0][i]\n",
    "        x = candidates [1][i]\n",
    "\n",
    "        t = max (0,y-30)\n",
    "        b = min (potential_fires.shape [0], y+31)\n",
    "        l = max (0, x-30)\n",
    "        r = min (potential_fires.shape [1], x+31)\n",
    "\n",
    "        eq4_result = r75 [y,x] > np.nanmean (r75_ignored [t:b,l:r]) + np.maximum (3 * (np.nanstd (r75_ignored [t:b,l:r])), 0.8)\n",
    "        eq5_result = bands [7][y,x] > np.nanmean (band7_ignored [t:b,l:r]) + np.maximum (3 * (np.nanstd (band7_ignored [t:b,l:r])), 0.08)\n",
    "        if not (eq4_result) or not (eq5_result):\n",
    "            potential_fires [y,x] = False\n",
    "\n",
    "    return potential_fires\n",
    "\n",
    "def Seq6 (bands):\n",
    "    p6 = np.where (bands[6] == 0, np.finfo (float).eps, bands[6])\n",
    "    return (bands [7] / p6 > 1.6)\n",
    "\n",
    "def Seq7_8_9 (bands):\n",
    "    result7 = np.logical_and (bands [4] > bands [5], np.logical_and (bands [5] > bands [6], np.logical_and (bands [6] > bands [7], bands [1] - bands [7] < 0.2)))\n",
    "    return (np.logical_and (result7, np.logical_or (bands [3] > bands [2], np.logical_and (bands [1] > bands [2], np.logical_and (bands [2] > bands [3], bands [3] > bands [4])))))\n",
    "\n",
    "\n",
    "def Geq12 (bands):\n",
    "    return (bands [4] <= 0.53 * bands [7] - 0.214)\n",
    "\n",
    "def Geq13 (bands, eq12_mask):\n",
    "    neighborhood = cv2.dilate (eq12_mask.astype (np.uint8), cv2.getStructuringElement (cv2.MORPH_RECT, (3,3))).astype (eq12_mask.dtype)\n",
    "\n",
    "    return (np.logical_and (neighborhood, bands [4] <= 0.35 * bands [6] - 0.044))\n",
    "\n",
    "def Geq14 (bands):\n",
    "    return (bands [4] <= 0.53 * bands [7] - 0.125)\n",
    "\n",
    "def Geq15 (bands):\n",
    "    return (bands [6] <= 1.08 * bands [7] - 0.048)\n",
    "\n",
    "def Geq16 (bands):\n",
    "    return (np.logical_and (np.logical_and (bands [2] > bands [3], bands [3] > bands [4]), bands [4] > bands [5]))\n",
    "\n",
    "def pixelVal(p7,ef,ep,ew):\n",
    "    e = np.logical_and (p7>0, np.logical_and (np.logical_not (ef), np.logical_and (np.logical_not (ep), np.logical_not (ew))))\n",
    "    return e\n",
    "\n",
    "def Geq8and9 (bands, valid, unamb_fires, potential_fires, water):\n",
    "\n",
    "    ignored_pixels = np.logical_or (unamb_fires, np.logical_or (potential_fires, water))\n",
    "    ignored_pixels = np.logical_or (ignored_pixels, np.logical_not (valid))\n",
    "    kept_pixels = np.logical_not (ignored_pixels)\n",
    "\n",
    "    r75 = bands [7] / bands [5]\n",
    "    r75_ignored = r75.copy ()\n",
    "    r75_ignored [ignored_pixels] = np.nan\n",
    "\n",
    "    band7_ignored = bands [7].copy ()\n",
    "    band7_ignored [ignored_pixels] = np.nan\n",
    "\n",
    "    sizes = list(range(5,61+2,2))\n",
    "\n",
    "    candidates = np.nonzero (potential_fires)\n",
    "\n",
    "    for i in range (len (candidates [0])):\n",
    "        y = candidates [0][i]\n",
    "        x = candidates [1][i]\n",
    "        tested = False\n",
    "        for w in sizes:\n",
    "            t = max (0,y-w//2)\n",
    "            b = min (potential_fires.shape [0], y+w//2+1)\n",
    "            l = max (0, x-w//2)\n",
    "            r = min (potential_fires.shape [1], x+w//2+1)\n",
    "\n",
    "            if np.count_nonzero (kept_pixels [t:b,l:r]) >= 0.25 * (b-t)*(r-l):\n",
    "                tested = True\n",
    "                eq8_result = r75 [y,x] > np.nanmean (r75_ignored [t:b,l:r]) + np.maximum (3 * (np.nanstd (r75_ignored [t:b,l:r])), 0.8)\n",
    "                eq9_result = bands [7][y,x] > np.nanmean (band7_ignored [t:b,l:r]) + np.maximum (3 * (np.nanstd (band7_ignored [t:b,l:r])), 0.08)\n",
    "                if not (eq8_result) or not (eq9_result):\n",
    "                    potential_fires [y,x] = False\n",
    "                break\n",
    "\n",
    "        if not tested:\n",
    "            potential_fires [y,x] = False\n",
    "\n",
    "    return potential_fires\n",
    "\n",
    "def Meq2 (bands):\n",
    "\n",
    "    p5 = np.where (bands[5] == 0, np.finfo (float).eps, bands[5])\n",
    "    p6 = np.where (bands[6] == 0, np.finfo (float).eps, bands[6])\n",
    "    \n",
    "    return (np.logical_and (bands[7] >= 0.15, np.logical_and (bands[7]/p6 >= 1.4, bands[7]/p5 >= 1.4)))\n",
    "\n",
    "def Meq3 (bands, unamb, sat):\n",
    "\n",
    "    neighborhood = cv2.dilate (unamb.astype (np.uint8), cv2.getStructuringElement (cv2.MORPH_RECT, (3,3))).astype (unamb.dtype)\n",
    "    p5 = np.where (bands[5] > 0, np.finfo (float).eps, bands[5])\n",
    "    \n",
    "    return (np.logical_and (neighborhood, np.logical_or (np.logical_and (bands[6]/p5 >= 2.0, bands[6]>=0.5), sat)))\n",
    "\n",
    "def getFireGOLI (bands):\n",
    "\n",
    "    valid = bands [7] > 0\n",
    "    valid = cv2.erode (valid.astype (np.uint8), cv2.getStructuringElement (cv2.MORPH_RECT, (3,3))).astype (np.uint8)\n",
    "\n",
    "    unamb_fires = Geq12 (bands)\n",
    "    unamb_fires = np.logical_and (valid, unamb_fires)\n",
    "    if np.any (unamb_fires):\n",
    "        unamb_fires = np.logical_or (unamb_fires, Geq13 (bands, unamb_fires))\n",
    "        unamb_fires = np.logical_and (valid, unamb_fires)\n",
    "\n",
    "    potential_fires = Geq14 (bands)\n",
    "    potential_fires = np.logical_or (potential_fires, Geq15 (bands))\n",
    "    potential_fires = np.logical_and (valid, potential_fires)\n",
    "\n",
    "    water = Geq16 (bands)\n",
    "\n",
    "    if np.any (potential_fires):\n",
    "        potential_fires = Geq8and9 (bands, valid, unamb_fires, potential_fires, water)\n",
    "\n",
    "    scaled_band = np.logical_and (np.logical_or (unamb_fires, potential_fires), np.logical_not (water))\n",
    "    return (scaled_band.astype (int))\n",
    "\n",
    "def getFireMurphy (bands, saturated):\n",
    "    unamb_fires = Meq2 (bands)\n",
    "\n",
    "    if np.any (unamb_fires):\n",
    "        potential_fires = Meq3 (bands, unamb_fires, saturated)\n",
    "        scaled_band = (unamb_fires | potential_fires)\n",
    "    else:\n",
    "        scaled_band = unamb_fires\n",
    "\n",
    "    return (scaled_band.astype (int))\n",
    "\n",
    "def getFireSchroeder (bands):\n",
    "    r75 = bands [7] / bands [5]\n",
    "    diff75 = bands [7] - bands [5]\n",
    "\n",
    "    unamb_fires = Seq1 (bands, r75, diff75)\n",
    "    unamb_fires = np.logical_or (unamb_fires, Seq2 (bands))\n",
    "\n",
    "    potential_fires = Seq3 (r75, diff75)\n",
    "\n",
    "    potential_fires = np.logical_and (potential_fires, Seq6 (bands))\n",
    "\n",
    "    water = Seq7_8_9 (bands)\n",
    "\n",
    "    if np.any (potential_fires):\n",
    "        potential_fires = Seq4and5 (bands, r75, unamb_fires, potential_fires, water)\n",
    "\n",
    "    scaled_band = np.logical_and (np.logical_or (unamb_fires, potential_fires), np.logical_not (water))\n",
    "    return (scaled_band.astype (int))\n",
    "\n",
    "def processImage (in_dir, image_name, Aref, Mref, SE, sat):\n",
    "    bands = np.zeros((8, 256, 256))\n",
    "    \n",
    "    with rasterio.open (os.path.join (in_dir, image_name + '.tif')) as src:\n",
    "        profile = src.profile.copy ()\n",
    "        for i in range (8):\n",
    "            if i == 0:\n",
    "                pass\n",
    "            else:\n",
    "                bands[i] = src.read (i)\n",
    "                \n",
    "    reflectance = np.copy(bands)\n",
    "    corrected = np.copy(bands)\n",
    "    \n",
    "    for i in range (1,8):\n",
    "        reflectance[i], corrected[i] = getReflectance (bands[i], Aref[i-1], Mref[i-1], SE)\n",
    "        \n",
    "    scaled_schroeder = getFireSchroeder(reflectance)\n",
    "    scaled_goli = getFireGOLI(corrected)\n",
    "    scaled_murphy = getFireMurphy(corrected, sat)\n",
    "\n",
    "    scaled_ir_band = [scaled_schroeder, scaled_goli, scaled_murphy]\n",
    "    \n",
    "    return np.array(scaled_ir_band)\n",
    "\n",
    "def voted_image(scaled_ir_band):\n",
    "    ir_voted_band = np.sum(scaled_ir_band, axis=0) >= 2\n",
    "    ir_voted_band = ir_voted_band.astype(np.uint8)\n",
    "    \n",
    "    return ir_voted_band\n",
    "\n",
    "def bands_combine(in_dir, image_name, scaling_band, g_band, b_band):\n",
    "    tif_file = os.path.join (in_dir, image_name + '.tif')\n",
    "    tif_data = gdal.Open(tif_file)\n",
    "    \n",
    "    band_b = (tif_data.GetRasterBand(1).ReadAsArray() / 256).astype(np.uint8)\n",
    "    band_g = (tif_data.GetRasterBand(2).ReadAsArray() / 256).astype(np.uint8)\n",
    "    band_r = scaling_band * 255\n",
    "    \n",
    "    rgb_image = np.dstack((band_b, band_g, band_r)).astype(np.uint8)\n",
    "    \n",
    "    return rgb_image\n",
    "\n",
    "def adjust_contrast(image, contrast=1.0):\n",
    "    f = 131 * (contrast + 127) / (127 * (131 - contrast))\n",
    "    alpha_c = f\n",
    "    gamma_c = 127 * (1 - f)\n",
    "    return cv2.addWeighted(image, alpha_c, image, 0, gamma_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataGenerator(Dataset):\n",
    "    def __init__(self, images_dir, transform=None, percent=1, ASSUMED_SE=55, ASSUMED_MREF=None, ASSUMED_AREF=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.images = sorted([x for x in os.listdir(images_dir) if x.endswith('.tif')])[:int(len(os.listdir(images_dir)) * percent)]\n",
    "        self.transform = transform\n",
    "        self.ASSUMED_SE = ASSUMED_SE\n",
    "        self.ASSUMED_MREF = [2e-05] * 8 if ASSUMED_MREF is None else ASSUMED_MREF\n",
    "        self.ASSUMED_AREF = [-0.1] * 8 if ASSUMED_AREF is None else ASSUMED_AREF\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tif_path = os.path.join(self.images_dir, self.images[idx])\n",
    "\n",
    "        image_name = os.path.basename(tif_path.replace('.tif',''))\n",
    "        with rasterio.open(tif_path) as src:\n",
    "            profile = src.profile.copy()\n",
    "            BQA = src.read(1)\n",
    "        saturation = get_saturation(BQA)  \n",
    "        scaling_band_3ch = processImage(self.images_dir, image_name, self.ASSUMED_AREF, self.ASSUMED_MREF, self.ASSUMED_SE, saturation)\n",
    "        scaling_band = voted_image(scaling_band_3ch)\n",
    "        img_array = bands_combine(self.images_dir, image_name, scaling_band, 2, 1)\n",
    "        img_array = adjust_contrast(img_array, contrast=1.3)\n",
    "        img = Image.fromarray(img_array).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        png_filename = self.images[idx].replace('.tif', '.png')\n",
    "\n",
    "        return {'image': img, 'name': png_filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out, dilation=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channel_in, channel_out, kernel_size=3, padding='same', dilation=dilation)\n",
    "        self.conv2 = nn.Conv2d(channel_out, channel_out, kernel_size=3, padding='same', dilation=dilation)\n",
    "        self.bnorm1 = nn.BatchNorm2d(channel_out)\n",
    "        self.bnorm2 = nn.BatchNorm2d(channel_out)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv1 = self.conv1(x)\n",
    "        conv1 = self.activation(self.bnorm1(conv1))\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv2 = self.activation(self.bnorm2(conv2))\n",
    "        return conv2\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(self, channel_in):\n",
    "        super().__init__()\n",
    "        self.downsample = nn.Conv2d(channel_in, channel_in * 2, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.downsample(x)\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(self, channel_in, channel_out):\n",
    "        super().__init__()\n",
    "        self.conv_transpose = nn.ConvTranspose2d(channel_in, channel_out, kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_transpose(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, clannels, classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.CHANNELS = clannels\n",
    "        self.CLASSES = classes\n",
    "\n",
    "        self.inp = ConvBlock(self.CHANNELS, 64)\n",
    "\n",
    "        self.stage1 = ConvBlock(128, 128, dilation=1)\n",
    "        self.stage2 = ConvBlock(256, 256, dilation=1)\n",
    "        self.stage3 = ConvBlock(512, 512, dilation=2)\n",
    "        self.stage4 = ConvBlock(1024, 1024, dilation=3)\n",
    "\n",
    "        self.down1 = Downsample(64)\n",
    "        self.down2 = Downsample(128)\n",
    "        self.down3 = Downsample(256)\n",
    "        self.down4 = Downsample(512)\n",
    "\n",
    "        self.up1 = Upsample(1024, 512)\n",
    "        self.up2 = Upsample(512, 256)\n",
    "        self.up3 = Upsample(256, 128)\n",
    "        self.up4 = Upsample(128, 64)\n",
    "\n",
    "        self.stage4i = ConvBlock(1024, 512, dilation=3)\n",
    "        self.stage3i = ConvBlock(512, 256, dilation=2)\n",
    "        self.stage2i = ConvBlock(256, 128, dilation=1)\n",
    "        self.stage1i = ConvBlock(128, 64, dilation=1)\n",
    "\n",
    "        self.out = nn.Conv2d(64, self.CLASSES, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.inp(x)\n",
    "        d1 = self.down1(a1)\n",
    "\n",
    "        a2 = self.stage1(d1)\n",
    "        d2 = self.down2(a2)\n",
    "\n",
    "        a3 = self.stage2(d2)\n",
    "        d3 = self.down3(a3)\n",
    "\n",
    "        a4 = self.stage3(d3)\n",
    "        d4 = self.down4(a4)\n",
    "\n",
    "        a5 = self.stage4(d4)\n",
    "        u1 = self.up1(a5)\n",
    "\n",
    "        c1 = self.stage4i(torch.cat([a4, u1], dim=1))\n",
    "        u2 = self.up2(c1)\n",
    "\n",
    "        c2 = self.stage3i(torch.cat([a3, u2], dim=1))\n",
    "        u3 = self.up3(c2)\n",
    "\n",
    "        c3 = self.stage2i(torch.cat([a2, u3], dim=1))\n",
    "        u4 = self.up4(c3)\n",
    "\n",
    "        c4 = self.stage1i(torch.cat([a1, u4], dim=1))\n",
    "        logits = self.out(c4)\n",
    "\n",
    "        return logits\n",
    "\n",
    "def unet(n_channels=3, n_classes=1):\n",
    "    return UNet(n_channels, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(tensor, filename, suffix):\n",
    "    filename = os.path.join(predict_mask_path, f\"{filename.split('.')[0]}.png\")\n",
    "    utils.save_image(tensor, filename)\n",
    "\n",
    "def predict(model):\n",
    "    testDataset = CustomDataGenerator(\n",
    "        images_dir=predict_img_path,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    testLoader = DataLoader(testDataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(testLoader):\n",
    "            images = batch['image'].to(device)\n",
    "            filenames = batch['name']\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # 예측 결과 저장\n",
    "            for i, filename in enumerate(filenames):\n",
    "                save_image(outputs[i].cpu(), filename, 'pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: cuda\n"
     ]
    }
   ],
   "source": [
    "# wandb.init(project=wandb_project_name)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [02:47<00:00,  1.50it/s]\n"
     ]
    }
   ],
   "source": [
    "model = unet(n_channels=CHANNELS).to(device)\n",
    "# model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "if os.path.exists(predict_pth_path):\n",
    "    checkpoint = torch.load(predict_pth_path)\n",
    "    model.load_state_dict(checkpoint)\n",
    "else:\n",
    "    print(\"Not Exists Model\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "predict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
